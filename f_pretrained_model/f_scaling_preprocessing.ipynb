{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87220b16-cb5a-4ad1-89f5-741fe268d9b0",
   "metadata": {},
   "source": [
    "### Scaling Preprocessing\n",
    "- 0 ~ 1, -1 ~ 1, z-score 변환 중에서 한 개를 선택하여 범위를 축소하는 작업을 의미한다.\n",
    "- Pretrained Model은 주로 tf와 torch 프레임워크 방식을 사용한다.\n",
    "- tf는 -1 ~ 1, torch는 z-score 변환하는 것이 각 프레임워크의 전통이다.\n",
    "<sub>*dataframe으로 부터 generator를 만들수 있는데, generator 시 0~255 사이의 값만 있지 않기 때문에 다른 것도 할 수 있게 한다.  \n",
    "**preprocessing 시 albumentations 후 preprocessing 진행</sub>\n",
    "\n",
    "<img src=\"./images/scaling.png\" width=\"400\" style=\"margin-top:20px; margin-left: -50px\">\n",
    "<sub>*rgb 분포까지 맞춘다.</sub>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "882e5dd9-b1cf-4bb4-bd8a-2fb60f72f469",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_scaling(x, mode='tf'):\n",
    "    if mode == 'tf': # -1 ~ 1 scale\n",
    "        x = x/127.5\n",
    "        x -= 1.\n",
    "    \n",
    "    elif mode == 'torch': # z-score scale\n",
    "        x = x/255.\n",
    "        mean = [0.485, 0.456, 0.406]\n",
    "        std = [0.229, 0.224, 0.225]\n",
    "        \n",
    "        x[:, :, 0] = (x[:, :, 0] - mean[0])/std[0]\n",
    "        x[:, :, 1] = (x[:, :, 1] - mean[1])/std[1]\n",
    "        x[:, :, 2] = (x[:, :, 2] - mean[2])/std[2]\n",
    "        \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0286593c-7f41-4b0b-b03b-39b6ea196270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cane': 'dog', 'cavallo': 'horse', 'elefante': 'elephant', 'farfalla': 'butterfly', 'gallina': 'chicken', 'gatto': 'cat', 'mucca': 'cow', 'pecora': 'sheep', 'scoiattolo': 'squirrel', 'dog': 'cane', 'elephant': 'elefante', 'butterfly': 'farfalla', 'chicken': 'gallina', 'cat': 'gatto', 'cow': 'mucca', 'spider': 'ragno', 'squirrel': 'scoiattolo'}\n",
      "{'dog': 'cane', 'horse': 'cavallo', 'elephant': 'elefante', 'butterfly': 'farfalla', 'chicken': 'gallina', 'cat': 'gatto', 'cow': 'mucca', 'sheep': 'pecora', 'squirrel': 'scoiattolo', 'cane': 'dog', 'elefante': 'elephant', 'farfalla': 'butterfly', 'gallina': 'chicken', 'gatto': 'cat', 'mucca': 'cow', 'ragno': 'spider', 'scoiattolo': 'squirrel'}\n"
     ]
    }
   ],
   "source": [
    "# with문으로 파일을 열고 f로 할당\n",
    "with open('./datasets/animals/translate.py') as f:\n",
    "    # 파일을 내용을 읽어와 content에 저장\n",
    "    content = f.readline()\n",
    "    # 중괄호 사이의 문장을 찾아서 가져오고\n",
    "    # eval을 사용하여 문자열 안에 갇혀있는 문장을 실제 딕셔너리로 변환\n",
    "    contents1 = eval(content[content.index('{'):content.index('}') + 1])\n",
    "    # 키와 값을 반대로 변경하여 저장\n",
    "    contents2 = {v : k for k, v in contents1.items()}\n",
    "\n",
    "print(contents1, contents2, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b47828e-2f7b-4aa0-b1f4-f1cfea738f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import os\n",
    "\n",
    "# 이미지가 저장되어 있는 경로 지정\n",
    "root = './datasets/animals/original/'\n",
    "\n",
    "# 해당 경로를 통해 이미지 폴더를 찾아옴\n",
    "# glob: 파일 경로와 이름 패턴을 사용하여 파일을 찾을 수 있게 해주는 모듈\n",
    "# *: 모든 문자열을 대체\n",
    "directories = glob(os.path.join(root, '*'))\n",
    "\n",
    "# 원래 정방향인데 에러나면 역방향으로 찾음\n",
    "for directory in directories:\n",
    "    try:\n",
    "        # 디렉터리 이름을 해당하는 번역된 이름으로 변경\n",
    "        os.rename(directory, os.path.join(root, contents1[directory[directory.rindex('\\\\') + 1:]]))\n",
    "    except KeyError as e:\n",
    "        # KeyError가 발생하면 번역된 이름이 아닌 원래 이름으로 변경\n",
    "        os.rename(directory, os.path.join(root, contents2[directory[directory.rindex('\\\\') + 1:]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6034f4c5-ecc1-4097-b587-09b8064c27b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['butterfly', 'cat', 'chicken', 'cow', 'dog', 'elephant', 'horse', 'sheep', 'spider', 'squirrel']\n"
     ]
    }
   ],
   "source": [
    "# 이미지가 저장되어 있는 경로 지정\n",
    "root = './datasets/animals/original/'\n",
    "\n",
    "# 해당 경로를 통해 이미지 폴더를 찾아옴\n",
    "directories = glob(os.path.join(root, '*'))\n",
    "# 폴더 이름 저장할 초기 list 생성\n",
    "directory_names = []\n",
    "\n",
    "for directory in directories:\n",
    "    # 디렉토리의 이름을 찾아와서 list에 저장\n",
    "    # rindex: 문자열에서 특정 문자 또는 부분 문자열의 마지막으로 발생하는 인덱스를 반환하는 메서드\n",
    "    directory_names.append(directory[directory.rindex('\\\\') + 1:])\n",
    "\n",
    "print(directory_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fbcdf30d-e933-461c-a609-f2a20fa0440b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 이미지가 저장되어 있는 경로 지정\n",
    "# root = './datasets/animals/original/'\n",
    "\n",
    "# # 이미지 별 폴더 안 이미지들의 파일명 변경\n",
    "# for name in directory_names:\n",
    "#     # os.listdir(): 지정된 디렉토리 내의 파일 목록 가져오기\n",
    "#     # enumerate(): 함수를 사용하여 파일 목록에서 파일 이름과 해당 파일의 인덱스를 반환\n",
    "#     for i, file_name in enumerate(os.listdir(os.path.join(root, name))):\n",
    "#         # 이전 파일의 전체 경로\n",
    "#         old_file = os.path.join(root + name + '/', file_name)\n",
    "#         # 신규 파일 전체 경로 작성\n",
    "#         new_file = os.path.join(root + name + '/', name + str(i + 1) + '.png')\n",
    "\n",
    "#         # 이전 파일의 이름을 신규 파일로 변경\n",
    "#         os.rename(old_file, new_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5db77623-7079-48d7-9ad6-68f77b221f9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 26179 images belonging to 10 classes.\n",
      "{'butterfly': 0, 'cat': 1, 'chicken': 2, 'cow': 3, 'dog': 4, 'elephant': 5, 'horse': 6, 'sheep': 7, 'spider': 8, 'squirrel': 9}\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# 이미지의 픽셀 값을 0에서 255 사이에서 0에서 1 사이의 값으로 조정\n",
    "# ImageDataGenerator: 이미지 데이터를 증강하고 전처리하는 데 사용되는 클래스\n",
    "idg = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# 디렉토리에서 이미지를 가져와 배치로 변환\n",
    "# target_size: 픽셀 크기 지정(150*150)\n",
    "# batch_size: 매개변수는 각 배치에 포함될 이미지 수를 지정\n",
    "# class_mode: 분류 작업을 수행할 때 이미지 레이블을 생성하는 방법을 지정\n",
    "generator = idg.flow_from_directory(root, target_size=(150, 150), batch_size=32, class_mode='categorical')\n",
    "print(generator.class_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c357af3c-50e6-4433-9343-11ce1634a869",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'butterfly',\n",
       " 1: 'cat',\n",
       " 2: 'chicken',\n",
       " 3: 'cow',\n",
       " 4: 'dog',\n",
       " 5: 'elephant',\n",
       " 6: 'horse',\n",
       " 7: 'sheep',\n",
       " 8: 'spider',\n",
       " 9: 'squirrel'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 각 타겟의 타겟 이름을 담기 위해 key와 value의 순서 변경하여 타겟의 고유값을 저장\n",
    "target_name = {v: k for k, v in generator.class_indices.items()}\n",
    "target_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7bb23826-c5e6-4ae6-9b87-a7c2b4447381",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 타겟의 이름 담을 초기 list 선언\n",
    "target_names = []\n",
    "# 각 타겟의 인덱스를 확인하여 인덱스에 맞는 타겟 이름을 담아주기\n",
    "for target in generator.classes:\n",
    "    target_names.append(target_name[target])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74cc9197-827f-4529-8d07-ad68f243b911",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_paths</th>\n",
       "      <th>target_names</th>\n",
       "      <th>targets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>./datasets/animals/original/butterfly/butterfl...</td>\n",
       "      <td>butterfly</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>./datasets/animals/original/butterfly/butterfl...</td>\n",
       "      <td>butterfly</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>./datasets/animals/original/butterfly/butterfl...</td>\n",
       "      <td>butterfly</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>./datasets/animals/original/butterfly/butterfl...</td>\n",
       "      <td>butterfly</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>./datasets/animals/original/butterfly/butterfl...</td>\n",
       "      <td>butterfly</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26174</th>\n",
       "      <td>./datasets/animals/original/squirrel/squirrel9...</td>\n",
       "      <td>squirrel</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26175</th>\n",
       "      <td>./datasets/animals/original/squirrel/squirrel9...</td>\n",
       "      <td>squirrel</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26176</th>\n",
       "      <td>./datasets/animals/original/squirrel/squirrel9...</td>\n",
       "      <td>squirrel</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26177</th>\n",
       "      <td>./datasets/animals/original/squirrel/squirrel9...</td>\n",
       "      <td>squirrel</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26178</th>\n",
       "      <td>./datasets/animals/original/squirrel/squirrel9...</td>\n",
       "      <td>squirrel</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26179 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              file_paths target_names  targets\n",
       "0      ./datasets/animals/original/butterfly/butterfl...    butterfly        0\n",
       "1      ./datasets/animals/original/butterfly/butterfl...    butterfly        0\n",
       "2      ./datasets/animals/original/butterfly/butterfl...    butterfly        0\n",
       "3      ./datasets/animals/original/butterfly/butterfl...    butterfly        0\n",
       "4      ./datasets/animals/original/butterfly/butterfl...    butterfly        0\n",
       "...                                                  ...          ...      ...\n",
       "26174  ./datasets/animals/original/squirrel/squirrel9...     squirrel        9\n",
       "26175  ./datasets/animals/original/squirrel/squirrel9...     squirrel        9\n",
       "26176  ./datasets/animals/original/squirrel/squirrel9...     squirrel        9\n",
       "26177  ./datasets/animals/original/squirrel/squirrel9...     squirrel        9\n",
       "26178  ./datasets/animals/original/squirrel/squirrel9...     squirrel        9\n",
       "\n",
       "[26179 rows x 3 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# read_csv 시 위 과정 생략하고 csv 읽어온 후 해당 코드 입력\n",
    "\n",
    "# 파일 경로와 타겟값을 가지고 새로운 데이터 프레임 생성\n",
    "animal_df = pd.DataFrame({'file_paths': generator.filepaths, 'target_names': target_names, 'targets': generator.classes})\n",
    "# 경로 중 \\\\(역슬래시)로 되어 있는 부분을 /(슬래시)로 변경 \n",
    "animal_df.file_paths = animal_df.file_paths.apply(lambda file_path: file_path.replace('\\\\', '/'))\n",
    "animal_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0dce7666-e498-42a8-b209-1deb5429150d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "targets\n",
      "4    3890\n",
      "8    3857\n",
      "2    2478\n",
      "6    2098\n",
      "0    1690\n",
      "3    1493\n",
      "9    1490\n",
      "7    1456\n",
      "1    1334\n",
      "5    1157\n",
      "Name: count, dtype: int64\n",
      "targets\n",
      "4    973\n",
      "8    964\n",
      "2    620\n",
      "6    525\n",
      "0    422\n",
      "3    373\n",
      "9    372\n",
      "7    364\n",
      "1    334\n",
      "5    289\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 데이터 세트 분리\n",
    "train_images, test_images, train_targets, test_targets = \\\n",
    "train_test_split(animal_df.file_paths, \n",
    "                 animal_df.targets, \n",
    "                 stratify=animal_df.targets, \n",
    "                 test_size=0.2, random_state=124)\n",
    "\n",
    "# 타겟 비중 확인\n",
    "print(train_targets.value_counts())\n",
    "print(test_targets.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "109a03d3-88d3-4a35-b9ec-dc65b97cd863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "targets\n",
      "4    3112\n",
      "8    3086\n",
      "2    1982\n",
      "6    1678\n",
      "0    1352\n",
      "3    1194\n",
      "9    1192\n",
      "7    1165\n",
      "1    1067\n",
      "5     926\n",
      "Name: count, dtype: int64\n",
      "targets\n",
      "4    778\n",
      "8    771\n",
      "2    496\n",
      "6    420\n",
      "0    338\n",
      "3    299\n",
      "9    298\n",
      "7    291\n",
      "1    267\n",
      "5    231\n",
      "Name: count, dtype: int64\n",
      "targets\n",
      "4    973\n",
      "8    964\n",
      "2    620\n",
      "6    525\n",
      "0    422\n",
      "3    373\n",
      "9    372\n",
      "7    364\n",
      "1    334\n",
      "5    289\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 검증용 데이터 세트 분리\n",
    "train_images, validation_images, train_targets, validation_targets = \\\n",
    "train_test_split(train_images, \n",
    "                 train_targets, \n",
    "                 stratify=train_targets, \n",
    "                 test_size=0.2, random_state=124)\n",
    "\n",
    "# 타겟 비중 확인\n",
    "print(train_targets.value_counts())\n",
    "print(validation_targets.value_counts())\n",
    "print(test_targets.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "28e3b739-55ba-4015-a88a-c94fca83ebcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16754, 3)\n",
      "(4189, 3)\n",
      "(5236, 3)\n"
     ]
    }
   ],
   "source": [
    "# 인덱스 번호를 대조하여 각 데이터 프레임 나누기\n",
    "train_df = animal_df.iloc[train_images.index].reset_index(drop=True)\n",
    "validation_df = animal_df.iloc[validation_images.index].reset_index(drop=True)\n",
    "test_df = animal_df.iloc[test_images.index].reset_index(drop=True)\n",
    "\n",
    "print(train_df.shape)\n",
    "print(validation_df.shape)\n",
    "print(test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a2d458ff-f32b-4564-8c51-bb9db4808c38",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'IMAGE_SIZE' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 19\u001b[0m\n\u001b[0;32m     13\u001b[0m test_generator \u001b[38;5;241m=\u001b[39m ImageDataGenerator(rescale\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m255\u001b[39m)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# 경로가 아닌 dataframe=train_df, x_col='file_paths', y_col='targets' 제너레이터로 불러오기\u001b[39;00m\n\u001b[0;32m     16\u001b[0m train_flow \u001b[38;5;241m=\u001b[39m train_generator\u001b[38;5;241m.\u001b[39mflow_from_dataframe(dataframe\u001b[38;5;241m=\u001b[39mtrain_df, \n\u001b[0;32m     17\u001b[0m                                                  x_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfile_paths\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m     18\u001b[0m                                                  y_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtargets\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m---> 19\u001b[0m                                                  target_size\u001b[38;5;241m=\u001b[39m(IMAGE_SIZE, IMAGE_SIZE), \n\u001b[0;32m     20\u001b[0m                                                  class_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m     21\u001b[0m                                                  shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     23\u001b[0m val_flow \u001b[38;5;241m=\u001b[39m val_generator\u001b[38;5;241m.\u001b[39mflow_from_dataframe(dataframe\u001b[38;5;241m=\u001b[39mval_df, \n\u001b[0;32m     24\u001b[0m                                                  x_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfile_paths\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m     25\u001b[0m                                                  y_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtargets\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m     26\u001b[0m                                                  target_size\u001b[38;5;241m=\u001b[39m(IMAGE_SIZE, IMAGE_SIZE), \n\u001b[0;32m     27\u001b[0m                                                  class_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     29\u001b[0m test_flow \u001b[38;5;241m=\u001b[39m test_generator\u001b[38;5;241m.\u001b[39mflow_from_dataframe(dataframe\u001b[38;5;241m=\u001b[39mtest_df, \n\u001b[0;32m     30\u001b[0m                                                  x_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfile_paths\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m     31\u001b[0m                                                  y_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtargets\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[0;32m     32\u001b[0m                                                  target_size\u001b[38;5;241m=\u001b[39m(IMAGE_SIZE, IMAGE_SIZE), \n\u001b[0;32m     33\u001b[0m                                                  class_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'IMAGE_SIZE' is not defined"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import albumentations as A\n",
    "\n",
    "# 이미지 사이즈 및 배치 사이즈\n",
    "IMAGE_SIZE = 64\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# 이미지 전처리 함수 선언\n",
    "def transform(image):\n",
    "    aug = A.HorizontalFlip(p=0.5)\n",
    "\n",
    "    return aug(image=image)['image']\n",
    "\n",
    "# 이미지 전처리 및 데이터 증강\n",
    "train_generator = ImageDataGenerator(preprocessing_function=transform, rescale=1./255)\n",
    "validation_generator = ImageDataGenerator(rescale=1./255)\n",
    "test_generator = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# 디렉토리에서 이미지를 가져와 배치로 변환\n",
    "# flow_from_dataframe에서 y_col에 들어갈 타겟 데이터는 문자열 타입만 가능하다.\n",
    "train_flow = train_generator.flow_from_dataframe(dataframe=train_df, \n",
    "                                                 x_col='file_paths', \n",
    "                                                 y_col='target_names',\n",
    "                                                 target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "                                                 class_mode='categorical',\n",
    "                                                 shuffle=True)\n",
    "\n",
    "validation_flow = validation_generator.flow_from_dataframe(dataframe=validation_df, \n",
    "                                                 x_col='file_paths', \n",
    "                                                 y_col='target_names',\n",
    "                                                 target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "                                                 class_mode='categorical')\n",
    "\n",
    "test_flow = test_generator.flow_from_dataframe(dataframe=test_df, \n",
    "                                                 x_col='file_paths', \n",
    "                                                 y_col='target_names',\n",
    "                                                 target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "                                                 class_mode='categorical')\n",
    "\n",
    "# 클래스 인덱스 출력\n",
    "print(train_flow.class_indices)\n",
    "print(validation_flow.class_indices)\n",
    "print(test_flow.class_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15919754-d7c4-43aa-ad9b-38ec54323220",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense , Conv2D , Dropout , Flatten , Activation, MaxPooling2D , GlobalAveragePooling2D\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "# ResNet50V2: 층이 50층 깊이, 20층만 필요하면 blank block을 통해 나머지 30층 을 실행하지 않을 수 있다.\n",
    "from tensorflow.keras.applications import ResNet50V2\n",
    "# Xception: inception 업그레이드 버전\n",
    "from tensorflow.keras.applications import Xception\n",
    "\n",
    "\n",
    "def create_model(model_name='vgg16', verbose=False):\n",
    "    input_tensor = Input(shape=(IMAGE_SIZE, IMAGE_SIZE, 3))\n",
    "    if model_name == 'vgg16':\n",
    "        model = VGG16(input_tensor=input_tensor, include_top=False, weights='imagenet')\n",
    "    elif model_name == 'resnet50': # ResNet50, 74.9% ; ResNet50V2, 76.0%\n",
    "        model = ResNet50V2(input_tensor=input_tensor, include_top=False, weights='imagenet')\n",
    "    elif model_name == 'xception': # Inception을 기초로 한 모델\n",
    "        model = Xception(input_tensor=input_tensor, include_top=False, weights='imagenet')\n",
    "\n",
    "    x = model.output\n",
    "\n",
    "    # 분류기\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    if model_name != 'vgg16':\n",
    "        x = Dropout(rate=0.5)(x)\n",
    "    x = Dense(50, activation='relu')(x)\n",
    "    if model_name != 'vgg16':\n",
    "        x = Dropout(rate=0.5)(x)\n",
    "    output = Dense(10, activation='softmax', name='output')(x)\n",
    "    \n",
    "    model = Model(inputs=input_tensor, outputs=output)\n",
    "    \n",
    "    if verbose:\n",
    "        model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e816e7de-1e49-4e5a-990c-f70c7872d346",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "\n",
    "model = create_model(model_name='xception', verbose=True)\n",
    "model.compile(optimizer=Adam(), loss=CategoricalCrossentropy(), metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425957da-51c7-494a-a1d8-cda965421583",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
    "\n",
    "mcp_cb = ModelCheckpoint(\n",
    "    filepath=\"./callback_files/weights.{epoch:03d}-{val_loss:.4f}-{acc:.4f}.weights.h5\",\n",
    "    monitor='val_loss',\n",
    "    save_best_only=False,\n",
    "    save_weights_only=True,\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "rlr_cb = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.1,\n",
    "    patience=2,\n",
    "    mode='min'\n",
    ")\n",
    "\n",
    "ely_cb = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=4,\n",
    "    mode='min'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e843dd-b944-4a6b-848b-51944c6aae66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0a5a2c5-e0cd-438e-a8b6-7075543e9fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 10\n",
    "\n",
    "history = model.fit(train_flow, \n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    epochs=N_EPOCHS, \n",
    "                    validation_data=validation_flow, \n",
    "                    callbacks=[mcp_cb, rlr_cb, ely_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7c06ea-02b8-46b5-a3e7-91984b8e7744",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(test_flow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93581913-eb7a-48b8-9061-ed495a3d8ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def show_history(history):\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.yticks(np.arange(0, 1, 0.05))\n",
    "    plt.plot(history.history['acc'], label='train')\n",
    "    plt.plot(history.history['val_acc'], label='validation')\n",
    "    plt.legend()\n",
    "    \n",
    "show_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eeb7379-1ea1-4c18-a373-24f978554601",
   "metadata": {},
   "source": [
    "### Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d70763c6-5842-450e-9fec-e45bd5aa5a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_scaling(image, mode='tf'):\n",
    "    if mode == 'tf': # -1 ~ 1 scale\n",
    "        image = image / 127.5\n",
    "        image -= 1.\n",
    "    \n",
    "    elif mode == 'torch': # z-score scale\n",
    "        image = image / 255.\n",
    "        mean = [0.485, 0.456, 0.406]\n",
    "        std = [0.229, 0.224, 0.225]\n",
    "        \n",
    "        image[:, :, 0] = (image[:, :, 0] - mean[0])/std[0]\n",
    "        image[:, :, 1] = (image[:, :, 1] - mean[1])/std[1]\n",
    "        image[:, :, 2] = (image[:, :, 2] - mean[2])/std[2]\n",
    "        \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb29ddc-8174-4dc8-b4bb-0e23ea89afb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "image = cv2.cvtColor(cv2.imread(train_df.file_paths.iloc[16749]), cv2.COLOR_BGR2RGB)\n",
    "plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2803503c-faf9-4588-a7c4-de1c3e8522b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 두개의 분포가 똑같지만 값의 범위가 다르다\n",
    "scaled_image_tf = preprocessing_scaling(image, mode='tf')\n",
    "scaled_image_torch = preprocessing_scaling(image, mode='torch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c7c90e-95a2-4fa1-bc52-415844b388f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_pixel_histogram(image):\n",
    "    fig, axs = plt.subplots(nrows=1, ncols=3, figsize=(12, 4))\n",
    "    titles = ['Red', 'Green', 'Blue']\n",
    "    for i in range(3):\n",
    "        axs[i].hist(image[:, :, i].flatten(), bins=100, alpha=0.5)\n",
    "        title_str = titles[i]\n",
    "        axs[i].set(title=title_str)\n",
    "\n",
    "show_pixel_histogram(scaled_image_tf)\n",
    "show_pixel_histogram(scaled_image_torch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466f2e03-03da-44b7-95a8-f81106816877",
   "metadata": {},
   "source": [
    "채널별 분포를 맞춘담에 배치 노말리제이션으로 또 하기 \n",
    "\n",
    "사전훈련모델은 분포가 마춰져 있고 우리가 어떤 방식으로 사용이 됐는지르 확인해봐야함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2ab6cf19-181f-4d74-b2ad-afea078a5e1e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'image' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapplications\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mxception\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m preprocess_input\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Xception의 scaling 방식은 tf\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# 적용한 preprocessing을 가져오기\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m scaled_image_xception \u001b[38;5;241m=\u001b[39m preprocess_input(image)\n\u001b[0;32m      6\u001b[0m show_picel_histogram(scaled_image_xception)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'image' is not defined"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications.xception import preprocess_input\n",
    "\n",
    "# Xception의 scaling 방식은 tf\n",
    "# 적용한 preprocessing을 가져오기\n",
    "scaled_image_xception = preprocess_input(image)\n",
    "show_pixel_histogram(scaled_image_xception)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4e3fa4-a68e-4978-925c-47804fc8798a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.densenet import preprocess_input\n",
    "\n",
    "# DenseNet의 scaling 방식은 torch\n",
    "scaled_image_densnet = preprocess_input(image)\n",
    "show_pixel_histogram(scaled_image_densnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e44e98-dc9f-4ba8-b99f-254b09586aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import albumentations as A\n",
    "\n",
    "IMAGE_SIZE = 64\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "def preprocessing_scaling_for_train(image, mode='tf'):\n",
    "    aug = A.HorizontalFlip(p=0.5)\n",
    "    image = aug(image=image)['image']\n",
    "    \n",
    "    if mode == 'tf': # -1 ~ 1 scale\n",
    "        image = image / 127.5\n",
    "        image -= 1.\n",
    "    \n",
    "    elif mode == 'torch': # z-score scale\n",
    "        image = image / 255.\n",
    "        mean = [0.485, 0.456, 0.406]\n",
    "        std = [0.229, 0.224, 0.225]\n",
    "        \n",
    "        image[:, :, 0] = (image[:, :, 0] - mean[0])/std[0]\n",
    "        image[:, :, 1] = (image[:, :, 1] - mean[1])/std[1]\n",
    "        image[:, :, 2] = (image[:, :, 2] - mean[2])/std[2]\n",
    "        \n",
    "    return image\n",
    "\n",
    "def preprocessing_scaling(image, mode='tf'):\n",
    "    if mode == 'tf': # -1 ~ 1 scale\n",
    "        image = image / 127.5\n",
    "        image -= 1.\n",
    "    \n",
    "    elif mode == 'torch': # z-score scale\n",
    "        image = image / 255.\n",
    "        mean = [0.485, 0.456, 0.406]\n",
    "        std = [0.229, 0.224, 0.225]\n",
    "        \n",
    "        image[:, :, 0] = (image[:, :, 0] - mean[0])/std[0]\n",
    "        image[:, :, 1] = (image[:, :, 1] - mean[1])/std[1]\n",
    "        image[:, :, 2] = (image[:, :, 2] - mean[2])/std[2]\n",
    "        \n",
    "    return image\n",
    "\n",
    "train_generator = ImageDataGenerator(preprocessing_function=preprocessing_scaling_for_train)\n",
    "validation_generator = ImageDataGenerator(preprocessing_function=preprocessing_scaling)\n",
    "test_generator = ImageDataGenerator(preprocessing_function=preprocessing_scaling)\n",
    "\n",
    "train_flow = train_generator.flow_from_dataframe(dataframe=train_df, \n",
    "                                                 x_col='file_paths', \n",
    "                                                 y_col='target_names',\n",
    "                                                 target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "                                                 class_mode='categorical',\n",
    "                                                 shuffle=True)\n",
    "\n",
    "validation_flow = validation_generator.flow_from_dataframe(dataframe=validation_df, \n",
    "                                                 x_col='file_paths', \n",
    "                                                 y_col='target_names',\n",
    "                                                 target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "                                                 class_mode='categorical')\n",
    "\n",
    "test_flow = test_generator.flow_from_dataframe(dataframe=test_df, \n",
    "                                                 x_col='file_paths', \n",
    "                                                 y_col='target_names',\n",
    "                                                 target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "                                                 class_mode='categorical')\n",
    "\n",
    "print(train_flow.class_indices)\n",
    "print(validation_flow.class_indices)\n",
    "print(test_flow.class_indices)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
